{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT_S2S_Attention_Per_Eng.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1PGxuxdA_1N2kYa9iSZLIFCdj0HtCBYtL",
      "authorship_tag": "ABX9TyMt1C4kcwdMMJ368M2JquOL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoyeBright/Multilingual_Multimodal_Universal_MT/blob/main/NMT_S2S_Attention_Per_Eng.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47SI7-dTSvb7"
      },
      "source": [
        "## **Hardware spec.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jsQQtyrRbDo",
        "outputId": "6b9689ad-f77d-41bc-eac3-2aa10bbe8623"
      },
      "source": [
        "!cat /proc/cpuinfo\n",
        "!cat /proc/meminfo"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2299.998\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs\n",
            "bogomips\t: 4599.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2299.998\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs\n",
            "bogomips\t: 4599.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "MemTotal:       13333596 kB\n",
            "MemFree:        10351456 kB\n",
            "MemAvailable:   12503820 kB\n",
            "Buffers:           93340 kB\n",
            "Cached:          2189972 kB\n",
            "SwapCached:            0 kB\n",
            "Active:           755348 kB\n",
            "Inactive:        1959328 kB\n",
            "Active(anon):     387688 kB\n",
            "Inactive(anon):      368 kB\n",
            "Active(file):     367660 kB\n",
            "Inactive(file):  1958960 kB\n",
            "Unevictable:           0 kB\n",
            "Mlocked:               0 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:               816 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:        431432 kB\n",
            "Mapped:           243576 kB\n",
            "Shmem:              1012 kB\n",
            "Slab:             174832 kB\n",
            "SReclaimable:     132936 kB\n",
            "SUnreclaim:        41896 kB\n",
            "KernelStack:        4480 kB\n",
            "PageTables:         5944 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6666796 kB\n",
            "Committed_AS:    3241516 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:           0 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:             1024 kB\n",
            "AnonHugePages:         0 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:       93416 kB\n",
            "DirectMap2M:     7245824 kB\n",
            "DirectMap1G:     8388608 kB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99p-OKLPSs0k"
      },
      "source": [
        "## **Import headers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZfqBktrTczU"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import io"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYlw1-9NazUq",
        "outputId": "099a9861-e7f3-4029-bf65-498276aeec77"
      },
      "source": [
        "!pip install hazm"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hazm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/13/5a7074bc11d20dbbb46239349ac3f85f7edc148b4cf68e9b8c2f8263830c/hazm-0.7.0-py3-none-any.whl (316kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 6.9MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1; platform_system != \"Windows\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/0f/1c9b49bb49821b5856a64ea6fac8d96a619b9f291d1f06999ea98a32c89c/libwapiti-0.2.1.tar.gz (233kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 26.3MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 34.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from libwapiti>=0.2.1; platform_system != \"Windows\"->hazm) (1.15.0)\n",
            "Building wheels for collected packages: libwapiti, nltk\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp36-cp36m-linux_x86_64.whl size=154555 sha256=9d883f02ef0b23f78bda7cd3b337249b207b82272d9f0439a98b3eed8d4d7be1\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/15/54/4510dce8bb958b1cdd2c47425cbd1e1eecc0480ac9bb1fb9ab\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-cp36-none-any.whl size=1394473 sha256=761d7623e94ec3f942081637ab4c30c77ce847ece0921b3c5d09e903017adc4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "Successfully built libwapiti nltk\n",
            "Installing collected packages: libwapiti, nltk, hazm\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rh6nrLka5Zk"
      },
      "source": [
        "from __future__ import unicode_literals\n",
        "from hazm import *"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xST28PQU58c"
      },
      "source": [
        "## **Mount corpora**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsyHzchDU4MF"
      },
      "source": [
        "file_path = '/content/drive/MyDrive/Multilingual_Multimodal_Unversal_MT/Parallel Corpora/Per-Eng/manythings/pes-eng/pes.txt'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rmtGQSpWIRk"
      },
      "source": [
        "## **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcSvWGE7Vzz1"
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn') \n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip()) # lowercase\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w) # creating a space between every word and punctuation\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w) # replacing every word with space except those defined\n",
        "\n",
        "  w = w.strip() # remove space from beginning and end of string\n",
        "  w = '<start> '+ w + ' <end>' # later model needs this to know where to start and stop\n",
        "\n",
        "  return w\n",
        "\n",
        "def preprocess_per_sentence(w):\n",
        "  normalizer = Normalizer() # Persian normalizer instance (Hazm)\n",
        "  normalizer.normalize(w)\n",
        "  w = unicode_to_ascii(w.lower().strip()) # lowercase\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = '<start> '+ w + ' <end>'\n",
        "\n",
        "  return w\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jwu0zIHeN97"
      },
      "source": [
        "## **An example**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_RB3V4QYL0q",
        "outputId": "a896d801-7bc3-42ec-f8cb-40ea0c506f2d"
      },
      "source": [
        "eng_sentence = \"You have beautiful hands.\"\n",
        "per_sentence =\"تو دستهای زیبایی داری.\"\n",
        "\n",
        "\n",
        "print(preprocess_sentence(eng_sentence))\n",
        "print(preprocess_per_sentence(per_sentence))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> you have beautiful hands . <end>\n",
            "<start> تو دستهای زیبایی داری .  <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqGOn_WcWboq"
      },
      "source": [
        "## **Create a dataset (word pair) from corpus**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGLaLO0leQeq"
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding=\"UTF-8\").read().strip().split('\\n')\n",
        "  word_pairs = [[preprocess_per_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs) # zip used for parallel iteration"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrjyp_mRSuLo"
      },
      "source": [
        "en, per, info = create_dataset(file_path, None) # info is an explanation which included in the corpus"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt5XhN3PDLaH",
        "outputId": "e83baf55-0a15-499f-f3bf-10b4ddc147da"
      },
      "source": [
        "print(en[-3])\n",
        "print(per[-3])\n",
        "print(info[-3])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> the difference between the right word and almost the right word is the difference between lightning and the lightning bug .  <end>\n",
            "<start> تفاوت بین کلمه صحیح و کلمه تقریبا صحیح مانند تفاوت بین مهتاب و کرم شب‌تاب است .  <end>\n",
            "<start> cc-by 2 . 0 (france) attribution: tatoeba . org #667975 (ck) & #7525272 (cojiluc) <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frvP8TX_WRox"
      },
      "source": [
        "def tokenize(lang):\n",
        "  language_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = '')\n",
        "  \n",
        "  # Creating the vocabulary index based on word frequency.\n",
        "  # 0 is a reserved index that won't be assigned to any word\n",
        "  language_tokenizer.fit_on_texts(lang) \n",
        "\n",
        "  # Transforming each text in texts to a sequence of integers\n",
        "  tensor = language_tokenizer.texts_to_sequences(lang) \n",
        "\n",
        "  # pads the sequences to the same length.\n",
        "  # seq = [[1], [2, 3], [4, 5, 6]]\n",
        "  # post-padding\n",
        "  # array([[1, 0, 0],\n",
        "  #          [2, 3, 0],\n",
        "  #          [4, 5, 6]])\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "  \n",
        "  return tensor, language_tokenizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnMUJ4JClpQC"
      },
      "source": [
        "def load_dataset(path, number_examples = None):\n",
        "  target_language, source_language, info = create_dataset(path, number_examples)\n",
        "\n",
        "  source_tensor, source_lang_tokenize = tokenize(source_language)\n",
        "  target_tensor, target_lang_tokenize = tokenize(target_language)\n",
        "\n",
        "  return source_tensor, target_tensor, source_lang_tokenize, target_lang_tokenize, info"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3ILFUvxnqJr"
      },
      "source": [
        "**Limit/ control the size of dataset to experiment faster**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTaJDsRLnobX"
      },
      "source": [
        "number_examples = 2200 # var\n",
        "source_tensor, target_tensor, source_lang, target_lang, info = load_dataset(file_path, number_examples)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCyI1JTX1pQZ"
      },
      "source": [
        "# Calculating max_length of the source and target tensors\n",
        "max_length_target, max_length_source = target_tensor.shape[1], source_tensor.shape[1]\n",
        "\n",
        "# Using sklearn to creat training and validation sets: 80-20 split\n",
        "source_tensor_train, source_tensor_val, target_tensor_train, target_tensor_val = train_test_split(source_tensor, target_tensor, test_size = 0.2)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNQPRA-e4KgY",
        "outputId": "2ff768ce-126a-4316-c529-21ee3325482d"
      },
      "source": [
        "# Print info up to here\n",
        "print(\" training size of source (Per): \", len(source_tensor_train),\"\\n\", \"training size of target (Eng): \", len(target_tensor_train), \"\\n\", \"validation size of source (Per): \", len(source_tensor_val), \"\\n\", \"validation size of target (Eng): \", len(target_tensor_val))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " training size of source (Per):  1760 \n",
            " training size of target (Eng):  1760 \n",
            " validation size of source (Per):  440 \n",
            " validation size of target (Eng):  440\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQef_1Ii_Nfk"
      },
      "source": [
        "### **Finding the word from their indices or vice versa (word-index dictionary)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuZ_aPR0_hKI"
      },
      "source": [
        "def convert(language, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print(\"%d ––––––> %s\" % (t, language.index_word[t]))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt3pdlGfByaM",
        "outputId": "a9a3b3b8-0bec-4d6b-dac0-1d48565461a0"
      },
      "source": [
        "print(\"mapping source language: index to word\")\n",
        "convert(source_lang, source_tensor_train[0])\n",
        "print(\"\")\n",
        "print(\"mapping target language: index to word\")\n",
        "convert(target_lang, target_tensor_train[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mapping source language: index to word\n",
            "1 ––––––> <start>\n",
            "6 ––––––> من\n",
            "402 ––––––> عادت\n",
            "4 ––––––> به\n",
            "852 ––––––> سخنرانی\n",
            "72 ––––––> کردن\n",
            "12 ––––––> در\n",
            "1238 ––––––> حضور\n",
            "1312 ––––––> جمع\n",
            "70 ––––––> ندارم\n",
            "3 ––––––> .\n",
            "2 ––––––> <end>\n",
            "\n",
            "mapping target language: index to word\n",
            "1 ––––––> <start>\n",
            "41 ––––––> i'm\n",
            "32 ––––––> not\n",
            "441 ––––––> used\n",
            "6 ––––––> to\n",
            "1983 ––––––> making\n",
            "1984 ––––––> speeches\n",
            "12 ––––––> in\n",
            "1050 ––––––> public\n",
            "3 ––––––> .\n",
            "2 ––––––> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBT8Njk4H8ts"
      },
      "source": [
        "## **Dataset preparation for Tensorflow**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqCVXCNqH5Lm",
        "outputId": "2155efeb-c9ab-41eb-f25d-be3973332cb5"
      },
      "source": [
        "#Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "step_per_epoch = len(source_tensor_train) // BATCH_SIZE\n",
        "embedding_dimension = 256\n",
        "units = 1024\n",
        "BUFFER_SIZE = len(source_tensor_train)\n",
        "vocabulary_source_size = len(source_lang.word_index) + 1\n",
        "vocabulary_target_size = len(target_lang.word_index) + 1\n",
        "\n",
        "print(\"Persian vocabulary size: \", vocabulary_source_size)\n",
        "print(\"English vocabulary size: \", vocabulary_target_size)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Persian vocabulary size:  3533\n",
            "English vocabulary size:  2438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1P4jQ2WX1u_"
      },
      "source": [
        "# Keras models accept three types of inputs: Numpy array, TF dataset objects(sth I used here), and Python generators\n",
        "dataset = tf.data.Dataset.from_tensor_slices((source_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "# drop_remainder ignores the last batch\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzaRIW_kydEf"
      },
      "source": [
        "### **An example**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztLONwXWeVOy",
        "outputId": "924566a8-da64-4bd8-c928-ec3c2ceece5d"
      },
      "source": [
        "example_source_batch, example_target_batch = next(iter(dataset))\n",
        "example_source_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 19]), TensorShape([64, 19]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDkzWA-Uyq0F"
      },
      "source": [
        "## **Designing encoder and decoder model with attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7UDvKLRyqGc"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocabulary_size, embedding_dimension, encoder_units, batch_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.encoder_units = encoder_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocabulary_size, embedding_dimension)\n",
        "    self.gru = tf.keras.layers.GRU(self.encoder_units, \n",
        "                                              return_sequences=True,\n",
        "                                              return_state=True,\n",
        "                                              recurrent_initializer='glorot_uniform') # default: orthogonal\n",
        "  \n",
        "  \n"
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}