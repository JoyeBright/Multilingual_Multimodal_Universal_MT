{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT_S2S_Attention_Per_Eng.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1PGxuxdA_1N2kYa9iSZLIFCdj0HtCBYtL",
      "authorship_tag": "ABX9TyOxPAqnmE2e5u89pEKtvLiW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoyeBright/Multilingual_Multimodal_Universal_MT/blob/main/NMT_S2S_Attention_Per_Eng.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47SI7-dTSvb7"
      },
      "source": [
        "## **Hardware spec.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jsQQtyrRbDo",
        "outputId": "0938477d-c872-454c-f8b5-83f8faa8fc71"
      },
      "source": [
        "!cat /proc/cpuinfo\n",
        "!cat /proc/meminfo"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2299.998\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs\n",
            "bogomips\t: 4599.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2299.998\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs\n",
            "bogomips\t: 4599.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "MemTotal:       13333596 kB\n",
            "MemFree:         9812552 kB\n",
            "MemAvailable:   12482284 kB\n",
            "Buffers:          128568 kB\n",
            "Cached:          2627748 kB\n",
            "SwapCached:            0 kB\n",
            "Active:          1075976 kB\n",
            "Inactive:        2156176 kB\n",
            "Active(anon):     398008 kB\n",
            "Inactive(anon):      364 kB\n",
            "Active(file):     677968 kB\n",
            "Inactive(file):  2155812 kB\n",
            "Unevictable:           0 kB\n",
            "Mlocked:               0 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:              1224 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:        475916 kB\n",
            "Mapped:           265580 kB\n",
            "Shmem:              1016 kB\n",
            "Slab:             195100 kB\n",
            "SReclaimable:     153236 kB\n",
            "SUnreclaim:        41864 kB\n",
            "KernelStack:        4544 kB\n",
            "PageTables:         6196 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6666796 kB\n",
            "Committed_AS:    3292592 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:           0 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:             1024 kB\n",
            "AnonHugePages:         0 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:       93416 kB\n",
            "DirectMap2M:     5148672 kB\n",
            "DirectMap1G:    10485760 kB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99p-OKLPSs0k"
      },
      "source": [
        "## **Import headers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZfqBktrTczU"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import io"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYlw1-9NazUq",
        "outputId": "6c2dc769-b734-4b3b-de33-ce4e6fd129b6"
      },
      "source": [
        "!pip install hazm"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hazm in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: libwapiti>=0.2.1; platform_system != \"Windows\" in /usr/local/lib/python3.6/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.6/dist-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from libwapiti>=0.2.1; platform_system != \"Windows\"->hazm) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rh6nrLka5Zk"
      },
      "source": [
        "from __future__ import unicode_literals\n",
        "from hazm import *"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xST28PQU58c"
      },
      "source": [
        "## **Mount corpora**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsyHzchDU4MF"
      },
      "source": [
        "file_path = '/content/drive/MyDrive/Multilingual_Multimodal_Unversal_MT/Parallel Corpora/Per-Eng/manythings/pes-eng/pes.txt'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rmtGQSpWIRk"
      },
      "source": [
        "## **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcSvWGE7Vzz1"
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn') \n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip()) # lowercase\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w) # creating a space between every word and punctuation\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w) # replacing every word with space except those defined\n",
        "\n",
        "  w = w.strip() # remove space from beginning and end of string\n",
        "  w = '<start> '+ w + ' <end>' # later model needs this to know where to start and stop\n",
        "\n",
        "  return w\n",
        "\n",
        "def preprocess_per_sentence(w):\n",
        "  normalizer = Normalizer() # Persian normalizer instance (Hazm)\n",
        "  normalizer.normalize(w)\n",
        "  w = unicode_to_ascii(w.lower().strip()) # lowercase\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = '<start> '+ w + ' <end>'\n",
        "\n",
        "  return w\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jwu0zIHeN97"
      },
      "source": [
        "## **An example**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_RB3V4QYL0q",
        "outputId": "4629a6c8-1350-4592-eb3b-384433f8dea1"
      },
      "source": [
        "eng_sentence = \"You have beautiful hands.\"\n",
        "per_sentence =\"تو دستهای زیبایی داری.\"\n",
        "\n",
        "\n",
        "print(preprocess_sentence(eng_sentence))\n",
        "print(preprocess_per_sentence(per_sentence))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> you have beautiful hands . <end>\n",
            "<start> تو دستهای زیبایی داری .  <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqGOn_WcWboq"
      },
      "source": [
        "## **Create a dataset (word pair) from corpus**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGLaLO0leQeq"
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding=\"UTF-8\").read().strip().split('\\n')\n",
        "  word_pairs = [[preprocess_per_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs) # zip used for parallel iteration"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrjyp_mRSuLo"
      },
      "source": [
        "en, per, info = create_dataset(file_path, None) # info is an explanation which included in the corpus"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt5XhN3PDLaH",
        "outputId": "a80474f1-3c8c-4732-f100-3b8a8deba398"
      },
      "source": [
        "print(en[-3])\n",
        "print(per[-3])\n",
        "print(info[-3])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> the difference between the right word and almost the right word is the difference between lightning and the lightning bug .  <end>\n",
            "<start> تفاوت بین کلمه صحیح و کلمه تقریبا صحیح مانند تفاوت بین مهتاب و کرم شب‌تاب است .  <end>\n",
            "<start> cc-by 2 . 0 (france) attribution: tatoeba . org #667975 (ck) & #7525272 (cojiluc) <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frvP8TX_WRox"
      },
      "source": [
        "def tokenize(lang):\n",
        "  language_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = '')\n",
        "  \n",
        "  # Creating the vocabulary index based on word frequency.\n",
        "  # 0 is a reserved index that won't be assigned to any word\n",
        "  language_tokenizer.fit_on_texts(lang) \n",
        "\n",
        "  # Transforming each text in texts to a sequence of integers\n",
        "  tensor = language_tokenizer.texts_to_sequences(lang) \n",
        "\n",
        "  # pads the sequences to the same length.\n",
        "  # seq = [[1], [2, 3], [4, 5, 6]]\n",
        "  # post-padding\n",
        "  # array([[1, 0, 0],\n",
        "  #          [2, 3, 0],\n",
        "  #          [4, 5, 6]])\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "  \n",
        "  return tensor, language_tokenizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnMUJ4JClpQC"
      },
      "source": [
        "def load_dataset(path, number_examples = None):\n",
        "  target_language, source_language, info = create_dataset(path, number_examples)\n",
        "\n",
        "  source_tensor, source_lang_tokenize = tokenize(source_language)\n",
        "  target_tensor, target_lang_tokenize = tokenize(target_language)\n",
        "\n",
        "  return source_tensor, target_tensor, source_lang_tokenize, target_lang_tokenize, info"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3ILFUvxnqJr"
      },
      "source": [
        "**Limit/ control the size of dataset to experiment faster**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTaJDsRLnobX"
      },
      "source": [
        "number_examples = 2200 # var\n",
        "source_tensor, target_tensor, source_lang, target_lang, info = load_dataset(file_path, number_examples)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCyI1JTX1pQZ"
      },
      "source": [
        "# Calculating max_length of the source and target tensors\n",
        "max_length_target, max_length_source = target_tensor.shape[1], source_tensor.shape[1]\n",
        "\n",
        "# Using sklearn to creat training and validation sets: 80-20 split\n",
        "source_tensor_train, source_tensor_val, target_tensor_train, target_tensor_val = train_test_split(source_tensor, target_tensor, test_size = 0.2)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNQPRA-e4KgY",
        "outputId": "50c973e8-bf8c-4cfa-9f99-6328b8789418"
      },
      "source": [
        "# Print info up to here\n",
        "print(\" training size of source (Per): \", len(source_tensor_train),\"\\n\", \"training size of target (Eng): \", len(target_tensor_train), \"\\n\", \"validation size of source (Per): \", len(source_tensor_val), \"\\n\", \"validation size of target (Eng): \", len(target_tensor_val))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " training size of source (Per):  1760 \n",
            " training size of target (Eng):  1760 \n",
            " validation size of source (Per):  440 \n",
            " validation size of target (Eng):  440\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQef_1Ii_Nfk"
      },
      "source": [
        "### **Finding the word from their indices**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuZ_aPR0_hKI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}